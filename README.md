# twitter_disaster
교과융합프로젝트 - 국가재난정보와 트위터 기반 머신러닝 분석

# 주요활동 내용
> + 재난정보와 트위터 글의 연관분석으로 재난 발생 예측이 가능하다는 가설을 세움. 
> + 파이썬의 기본 문법(변수 선언, if문, for문), pandas, tansorflow 기초, jupyter 사용법 학습함
> + 국가재난정보 엑셀 데이터를 pandas의 데이터프레임으로 변환 작업 학습함.
> + 실시간으로 트위터 글들을 검색하여 결과를 pandas의 데이터프레임으로 변환하는 작업
> + 국가재난정보의 시간 정보에 대응하는 트위터 글을 검색하여 위험도가 높은 재난과 낮은 재난정보 추출하는 작업
> + 재난 연관성이 있는 트위터 글과 낮은 글을 BERT 모형으로 학습하는 파이썬 코딩 일부 완성(미검증) 

# 연구의 결과
* 뉴스를 통해서 위험을 감지하고 예방 혹은 예측할 수 있는 가능성을 얻음.
1. 인공지능 모형의 입력값의 필요성
> + 인공지능 모형은 추론 가능한 가설을 만들어야 함
> + 가설: 뉴스를 통해서 위험을 감지하고 예방 혹은 예측할 수 있다
> + 뉴스의 작성 시점과 같은 정확한 시간 정보가 필요함
* 트위터처럼 즉시성을 갖는 뉴스를 통해 위험을 감지하는 것이 더 효과적임.
2. 예측 모형의 신뢰도 vs 신속성
> + 뉴스는 체계적으로 보도하기 위한 절차와 시간이 필요하기 때문에 신속성 떨어짐
> + 트위터는 신뢰도는 낮으나 재난 발생 시점에 가장 근접한 소식을 전달할 수 있다고 여김
> + 재난 상황에서는 신뢰도보다 신속성에 초점을 두어 대응하는 것이 중요
> + 신뢰도를 보완하기 위해서 과거 트위터 작성 시점에 재난발생 기록을 연동하여 재난 발생시 나타나는 트위터 글의 의미와 감성 패턴을 모형으로 구현
* 뉴스의 의미와 감성을 분석하는 방법으로 KoBERT(pre-trained된 학습결과를 사용하여 조금만 바꿔주는 fine-tuning 방식)를 활용.
3. 효율적인 의미와 감성 분석 방법
> + BERT:사전 훈련된 모형(ex: 위키, 백과사전...)을 조금만 튜닝해서 다른 용도로 사용하는 방법의 효율성을 얻음
> + BERT 모형을 통해서 학습해야하는 양을 줄이고 시간을 절약할 수 있음
* 인공지능 모형을 만들기 위해서는 "국가재난포털"의 재난유형 정보와 같은 정확한 "사실 데이터"가 필요함.
4. 사실 데이터 기반의 예측&예방
> + 국가재난포털의 "2020년 지자체 사고 8종 마이크로데이터-수난" 데이터를 사용하여 사고발생 시각에 작성된 트위터 글중 태풍이라는 단어가 들어가면  재난 위험도 점수를 높게 할당함
> + 사망자수*2 + 부상자수*1 = 위험도
> + 재난 발생시점과 연관된 글이 아닐 수 있으므로 태풍이라는 단어가 들어가지 않는 글에 위험도 0으로 부여해 학습
> + 서로 연관된 사실 데이터가 필요하므로 트위터 글 중 재난발생과 직접적인 연관관계가 있는 "장소" 정보를 활용할 수 있다면 예측의 정확도가 상승할 것으로 보임

# 연구의 시사점 및 제언

> + 데이터가 제공하는 이로운 점
> + 데이터가 없는 인공지능은 홍철없는 홍철팀
> + 데이터 축적에 힘쓰는 정부의 노력
> + 다른 분야에 활용

# 출처
> + KoBERT 샘플 참조 'https://github.com/monologg/KoBERT-nsmc'
> + 국가재난포털 참조 'https://www.safekorea.go.kr/idsiSFK/neo/sfk/cs/csc/bbs_conf.jsp?bbs_no=27&emgPage=Y&menuSeq=736&viewtype=read&bbs_ordr=2136'
> + 트위터 api 사용방법 참조 'https://www.youtube.com/watch?time_continue=2&v=Lu1nskBkPJU&feature=emb_title'
> + pandas 데이터프레임 사용방법 참조 'https://www.youtube.com/watch?v=pbHc9Kbztic'
